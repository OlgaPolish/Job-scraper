#!/usr/bin/env python
# coding: utf-8

# In[ ]:


import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import random
from datetime import datetime
from flask import Flask, render_template, request
import os
import shutil

# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É templates, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
if not os.path.exists('templates'):
    os.makedirs('templates')

# –ö–æ–ø–∏—Ä—É–µ–º index.html, –µ—Å–ª–∏ –µ–≥–æ –µ—â—ë –Ω–µ—Ç –≤ templates
if not os.path.exists('templates/index.html'):
    if os.path.exists('index.html'):
        shutil.copy('index.html', 'templates/index.html')
    else:
        # –ï—Å–ª–∏ index.html –Ω–µ—Ç —Ä—è–¥–æ–º —Å app.py, —Å–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç–æ–π —à–∞–±–ª–æ–Ω
        with open('templates/index.html', 'w', encoding='utf-8') as f:
            f.write('<h1>Welcome! index.html –Ω–µ –Ω–∞–π–¥–µ–Ω.</h1>')

app = Flask(__name__)

class LinkedInJobScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/91.0.4472.124 Safari/537.36'
            )
        }
        self.jobs_list = []

    def get_user_requirements(self, keywords, locations):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        return {
            'keywords': [kw.strip() for kw in keywords.split(',') if kw.strip()],
            'locations': [loc.strip() for loc in locations.split(',') if loc.strip()],
            'max_pages': 3  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        }

    def scrape_jobs(self, requirements):

        """–°–∫—Ä–∞–ø–∏–Ω–≥ –≤–∞–∫–∞–Ω—Å–∏–π —Å LinkedIn —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–π —Å—Å—ã–ª–∫–µ"""
        print("\nüîç –ù–ê–ß–ò–ù–ê–ï–ú –°–ö–†–ê–ü–ò–ù–ì –í–ê–ö–ê–ù–°–ò–ô...")

        self.jobs_list = []
    
        for city in requirements['locations']:
            for kw in requirements['keywords']:
                for start in range(0, requirements['max_pages'] * 25, 25):
                    try:
                        kw_encoded = kw.replace(" ", "%20")
                        city_encoded = city.replace(" ", "%20")
                        url = (
                            f'https://www.linkedin.com/jobs/search/?'
                            f'keywords={kw_encoded}&location={city_encoded}&start={start}'
                        )
                        print(f'üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞: {url}')
                        response = requests.get(url, headers=self.headers, timeout=15)
                        soup = BeautifulSoup(response.text, 'html.parser')
    
                        job_cards = soup.find_all('div', class_='base-card')
                        print(f'   ‚û§ –ù–∞–π–¥–µ–Ω–æ: {len(job_cards)} –≤–∞–∫–∞–Ω—Å–∏–π')
    
                        for job in job_cards:
                            job_data = self.extract_job_data(job, kw, city)
                            if job_data:
                                self.jobs_list.append(job_data)
    
                        time.sleep(random.uniform(2, 5))
    
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫—Ä–∞–ø–∏–Ω–≥–µ: {e}")
                        time.sleep(10)
    
        # --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Å—Å—ã–ª–∫–µ ---
        unique_links = set()
        unique_jobs = []
        for job in self.jobs_list:
            link = job.get('Link')
            if link and link not in unique_links:
                unique_jobs.append(job)
                unique_links.add(link)
        self.jobs_list = unique_jobs
    
        print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ {len(self.jobs_list)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π")

        return self.jobs_list

    def extract_job_data(self, job, keyword, city):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∫–∞–Ω—Å–∏–∏"""
        try:
            title = job.find('h3', class_='base-search-card__title')
            title = title.text.strip() if title else ''

            company = job.find('h4', class_='base-search-card__subtitle')
            company = company.text.strip() if company else ''

            location = job.find('span', class_='job-search-card__location')
            location = location.text.strip() if location else ''

            link_elem = job.find('a', class_='base-card__full-link')
            link = link_elem['href'] if link_elem else ''

            date_elem = job.find('time', class_='job-search-card__listdate')
            date_posted = date_elem.text.strip() if date_elem else '–ù–µ —É–∫–∞–∑–∞–Ω–æ'

            return {
                'Title': title,
                'Company': company,
                'Location': location,
                'Link': link,
                'Keyword': keyword,
                'Search_City': city,
                'Date_Posted': date_posted,
                'Date_Scraped': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None

    def fetch_job_descriptions(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª–Ω—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π –≤–∞–∫–∞–Ω—Å–∏–π"""
        print("\nüìù –ü–û–õ–£–ß–ï–ù–ò–ï –û–ü–ò–°–ê–ù–ò–ô –í–ê–ö–ê–ù–°–ò–ô...")
        descriptions = []
        salaries = []

        for i, job in enumerate(self.jobs_list):
            if not job['Link']:
                descriptions.append('–°—Å—ã–ª–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞')
                salaries.append('–ù–µ —É–∫–∞–∑–∞–Ω–æ')
                continue

            print(f"{i+1}/{len(self.jobs_list)} | {job['Title'][:50]}...")

            try:
                response = requests.get(job['Link'], headers=self.headers, timeout=12)
                soup = BeautifulSoup(response.text, 'html.parser')

                desc_block = soup.find(
                    'div',
                    {'class': lambda x: x and 'description' in x.lower() if x else False}
                )
                if not desc_block:
                    desc_block = soup.find(
                        'section',
                        {'class': lambda x: x and 'description' in x.lower() if x else False}
                    )

                if desc_block:
                    description = desc_block.get_text(separator=' ', strip=True)
                else:
                    description = '–û–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'

                salary = self.extract_salary(soup, description)

                descriptions.append(description)
                salaries.append(salary)

                time.sleep(random.uniform(1.5, 4))

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
                descriptions.append('–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏')
                salaries.append('–ù–µ —É–∫–∞–∑–∞–Ω–æ')

        return descriptions, salaries

    def extract_salary(self, soup, description):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–∞—Ä–ø–ª–∞—Ç–µ"""
        salary_patterns = [
            r'‚Ç¨\s*([0-9]{2,3}[.,]?[0-9]{3})[\\s-]*([0-9]{2,3}[.,]?[0-9]{3})?',
            r'([0-9]{2,3})[.,]?([0-9]{3})\s*‚Ç¨',
            r'salary.*?([0-9]{2,3}[.,]?[0-9]{3})',
            r'gehalt.*?([0-9]{2,3}[.,]?[0-9]{3})',
        ]

        text = description.lower()
        for pattern in salary_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(0)

        return '–ù–µ —É–∫–∞–∑–∞–Ω–æ'

    def analyze_with_ai(self, df, user_prompt, priority_keywords):
        """AI-–∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –≤–∞–∫–∞–Ω—Å–∏–π"""
        print("\nü§ñ AI-–ê–ù–ê–õ–ò–ó –ò –ü–†–ò–û–†–ò–¢–ò–ó–ê–¶–ò–Ø...")

        df['Brief_Description'] = df.apply(self.create_brief_description, axis=1)
        df['Skills_Match'] = df['Description'].apply(
            lambda x: self.calculate_skills_match(x, priority_keywords)
        )
        df['Remote_Work'] = df['Description'].apply(self.detect_remote_work)
        df['Seniority_Level'] = df['Description'].apply(self.detect_seniority)
        df['Language'] = df['Description'].apply(self.detect_language)
        df['Priority'] = df.apply(
            lambda row: self.calculate_priority(row, user_prompt, priority_keywords), axis=1
        )

        df = df.sort_values('Priority')
        return df

    def create_brief_description(self, row):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è –≤–∞–∫–∞–Ω—Å–∏–∏"""
        desc = row['Description'][:300] + "..." if len(row['Description']) > 300 else row['Description']
        return f"{row['Title']} –≤ {row['Company']}, {row['Location']}. {desc}"

    def calculate_skills_match(self, description, priority_keywords):
        """–ü–æ–¥—Å—á–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞–≤—ã–∫–æ–≤"""
        desc_lower = description.lower()
        matches = sum(1 for keyword in priority_keywords if keyword in desc_lower)
        return f"{matches}/{len(priority_keywords)}"

    def detect_remote_work(self, description):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É–¥–∞–ª–µ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã"""
        remote_keywords = [
            'remote', 'home office', 'hybrid', 'remotearbeit', 'homeoffice', '–æ—Ç –¥–æ–º–∞'
        ]
        desc_lower = description.lower()
        return any(keyword in desc_lower for keyword in remote_keywords)

    def detect_seniority(self, description):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –ø–æ–∑–∏—Ü–∏–∏"""
        desc_lower = description.lower()
        if any(word in desc_lower for word in ['senior', 'lead', 'principal', 'architect']):
            return 'Senior'
        elif any(word in desc_lower for word in ['junior', 'entry', 'anf√§nger']):
            return 'Junior'
        elif any(word in desc_lower for word in ['manager', 'head', 'director', 'leitung']):
            return 'Management'
        elif any(word in desc_lower for word in ['intern', 'praktikum', 'werkstudent']):
            return 'Intern/Student'
        else:
            return 'Mid-level'

    def detect_language(self, description):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —è–∑—ã–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–∏"""
        desc_lower = description.lower()
        english_words = ['responsibilities', 'requirements', 'experience', 'skills', 'apply']
        german_words = ['anforderungen', 'verantwortung', 'erfahrung', 'kenntnisse', 'bewerben']

        en_count = sum(1 for word in english_words if word in desc_lower)
        de_count = sum(1 for word in german_words if word in desc_lower)

        if en_count > de_count:
            return 'English'
        elif de_count > en_count:
            return 'German'
        else:
            return 'Mixed'

    def calculate_priority(self, row, user_prompt, priority_keywords):
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ AI-–ª–æ–≥–∏–∫–∏"""
        score = 0
        desc_lower = row['Description'].lower()
        title_lower = row['Title'].lower()

        skills_matched = sum(1 for keyword in priority_keywords if keyword in desc_lower)
        score += skills_matched * 10

        title_matches = sum(1 for keyword in priority_keywords if keyword in title_lower)
        score += title_matches * 15

        prompt_words = user_prompt.lower().split()
        prompt_matches = sum(1 for word in prompt_words if len(word) > 3 and word in desc_lower)
        score += prompt_matches * 5

        if row['Remote_Work'] and any(word in user_prompt.lower() for word in ['remote', '—É–¥–∞–ª–µ–Ω', '–¥–æ–º']):
            score += 20

        if 'Salary' in row and row['Salary'] != '–ù–µ —É–∫–∞–∑–∞–Ω–æ':
            score += 10

        if row['Seniority_Level'] == 'Senior' and 'junior' in user_prompt.lower():
            score -= 15
        elif row['Seniority_Level'] == 'Junior' and 'senior' in user_prompt.lower():
            score -= 15

        if score >= 50:
            return 1
        elif score >= 25:
            return 2
        else:
            return 3

    def save_results(self, df):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        columns_order = [
            'Priority', 'Title', 'Company', 'Location', 'Brief_Description',
            'Skills_Match', 'Salary', 'Remote_Work', 'Seniority_Level',
            'Language', 'Date_Posted', 'Link', 'Description'
        ]

        for col in columns_order:
            if col not in df.columns:
                df[col] = '–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ'

        df_ordered = df[columns_order]

        excel_filename = f"linkedin_jobs_ai_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        df_ordered.to_excel(excel_filename, index=False)

        csv_filename = excel_filename.replace('.xlsx', '.csv')
        df_ordered.to_csv(csv_filename, index=False)

        print(f"\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–•–†–ê–ù–ï–ù–´:")
        print(f"üìä Excel —Ñ–∞–π–ª: {excel_filename}")
        print(f"üìÑ CSV —Ñ–∞–π–ª: {csv_filename}")
        print(f"üìà –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {len(df)}")

        priority_stats = df['Priority'].value_counts().sort_index()
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú:")
        for priority, count in priority_stats.items():
            priority_name = {1: '–í—ã—Å–æ–∫–∏–π', 2: '–°—Ä–µ–¥–Ω–∏–π', 3: '–ù–∏–∑–∫–∏–π'}.get(priority, '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω')
            print(f"   –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç {priority} ({priority_name}): {count} –≤–∞–∫–∞–Ω—Å–∏–π")

        return excel_filename, csv_filename

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/scrape', methods=['POST'])
def scrape():
    scraper = LinkedInJobScraper()
    keywords = request.form['keywords']
    locations = request.form['locations']
    max_pages = int(request.form.get('max_pages', 3))  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3
    
    requirements = scraper.get_user_requirements(keywords, locations)
    requirements['max_pages'] = max_pages  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü

    jobs = scraper.scrape_jobs(requirements)
    if not jobs:
        return "‚ùå –í–∞–∫–∞–Ω—Å–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    descriptions, salaries = scraper.fetch_job_descriptions()
    df = pd.DataFrame(jobs)
    df['Description'] = descriptions
    df['Salary'] = salaries

    user_prompt = request.form.get('user_prompt', '')
    priority_keywords = [
        kw.strip().lower() for kw in request.form.get('priority_keywords', '').split(',') if kw.strip()
    ]

    df_analyzed = scraper.analyze_with_ai(df, user_prompt, priority_keywords)
    excel_file, csv_file = scraper.save_results(df_analyzed)

    return f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {excel_file} –∏ {csv_file}."

if __name__ == "__main__":
    app.run(debug=True)



# In[ ]:




